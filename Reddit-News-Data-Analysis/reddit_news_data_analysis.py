# -*- coding: utf-8 -*-
"""Reddit-News-Data-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZeId1gC--C33HgOJgSifMrteXI6JqF3

# Reddit-News Data Analyzer

The following notebook will focus on doing some data manipulation and data analysis on Reddit-News data from 2008-2016. The notebook is split into several sections to keep it organized and show the user clear steps in running the application



1. Library Installations
2. Download data
3. Initial insights into the data
4. Top 10 Topics discussed
5. Count of good news and bad news

For Section 4, I used the TextBlob library to perform sentiment analysis on the news data sent. I used both the Pyspark DF library as well as Pandas DF. Pandas DF was easier to use and manipulate for sentiment analysis but Spark provides parallel processing on different nodes in the cluster.

# Library Installations
"""

!pip install gensim

!pip install nltk

!pip install TextBlob

"""## Install PySpark library"""

!pip install pyspark
import os
import sys
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext, SparkSession
import pyspark.sql.functions as f

import os
!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
!tar -xvf /content/spark-3.0.1-bin-hadoop2.7.tgz
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))
spark = SparkSession \
    .builder \
    .getOrCreate()

"""## Get Reddit-News data from Github"""

!wget -q https://raw.githubusercontent.com/ashfarhangi/Massive_Storage_and_Big_Data/master/data/Reddit-News.csv

"""# Insights about the Data"""

import pyspark.sql.functions as f

reddit_news_df = spark.read.csv('Reddit-News.csv', inferSchema=True, header=True)

reddit_news_df.printSchema()
print(reddit_news_df.describe())
reddit_news_df.show()

"""Spark helps us gain some initial insights about the data by printing the schema, summary and then display the dataframe.

We can see that the data has two columns "Date" and "News" that both contain Strings. Each row contains the news headline and the respective date of the headline.

# Top 10 most discussed topics from 2008-2016
"""

import gensim

# create list of stop words to use for filtering
stop_words = gensim.parsing.preprocessing.STOPWORDS.union(set(['new', 'news', 'says']))

# read in the reddt-news file and split it by line 
reddit_news = sc.textFile('Reddit-News.csv').map(lambda line: line.split(',', 1)[-1])

# take each line in the RDD and split it by white space to get individual words
words = reddit_news.flatMap(lambda line: line.lower().split(' '))

# filter stopwords out and get count of all word occurences 
word_count = words.filter(lambda word: word not in stop_words and len(word) > 2).map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)

# sort by count and take the top 10 entries
most_common_words = word_count.map(lambda pair: (pair[1], pair[0])).sortByKey(False).take(10)

# display the top 10 most discussed topics
print('The top 10 most discussed topics are:')
for count,pair in enumerate(most_common_words):
  print('#{} topic: "{}" with {} occurences'.format(count+1, pair[-1], pair[0]))

"""# Senitment Analysis using TextBlob library

The following Sentiment Analysis of the Reddit-News data uses the TextBlob library and Pandas dataframe as well as the Pyspark dataframe with TextBlob for comparison

## Using Pandas Dataframe with TextBlob library for sentiment analysis
"""

import pandas as pd
from textblob import TextBlob

# read in the reddit-news as a pandas dataframe 
reddit_news_df = pd.read_csv('Reddit-News.csv', parse_dates=True, index_col='Date')

# apply the TextBlob sentiment analysis to each row containing news headline
reddit_news_df['Sentiment Score'] = reddit_news_df['News'].apply(lambda headline: TextBlob(headline).sentiment.polarity) 
display(reddit_news_df)

# get good and bad news count for each sentiment score
sentiment_scores = reddit_news_df['Sentiment Score']
good_news_count = reddit_news_df[reddit_news_df['Sentiment Score'] > 0].count()
bad_news_count = reddit_news_df[reddit_news_df['Sentiment Score'] < 0].count()
print('Good news count: \n{}\n'.format(good_news_count))
print('Bad news count: \n{}\n'.format(bad_news_count))

"""## Using PySpark dataframes with TextBlob library for sentiment analysis"""

from textblob import TextBlob

# function used to return the sentiment of the passed in news headline
def find_sentiment(news_headline):
  sentiment_score = TextBlob(news_headline).sentiment.polarity
  return sentiment_score

from pyspark.sql.types import DoubleType

# read in reddit-news and display schema info
reddit_news_df = spark.read.csv('Reddit-News.csv', inferSchema=True, header=True)
reddit_news_df.printSchema()
news_data = reddit_news_df.select('News')

# create a user-defined function that will apply find_sentiment to passed in headlines
sentiment_udf = f.udf(find_sentiment, DoubleType())
spark.udf.register('sentiment', sentiment_udf)

# get sentiment scores and create new column with sentiment scores for each headline
news_data_with_sentiment = reddit_news_df.withColumn('Sentiment Score', sentiment_udf('News').cast('double'))
news_data_with_sentiment.show()