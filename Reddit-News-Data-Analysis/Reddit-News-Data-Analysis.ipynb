{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzD299LNu-TA",
        "colab_type": "text"
      },
      "source": [
        "# Reddit-News Data Analyzer\n",
        "\n",
        "The following notebook will focus on doing some data manipulation and data analysis on Reddit-News data from 2008-2016. The notebook is split into several sections to keep it organized and show the user clear steps in running the application\n",
        "\n",
        "\n",
        "\n",
        "1. Library Installations\n",
        "2. Initial insights into the data\n",
        "3. Top 10 Topics discussed\n",
        "4. Count of good news and bad news\n",
        "\n",
        "For Section 4, I used the TextBlob library to perform sentiment analysis on the news data sent. I used both the Pyspark DF library as well as Pandas DF. Pandas DF was easier to use and manipulate for sentiment analysis but Spark provides parallel processing on different nodes in the cluster. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwInyTF5qQ-F",
        "colab_type": "text"
      },
      "source": [
        "# Library Installations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al1n0IqHK3mT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c4a2e3ed-b379-4890-9bae-25bdf0676484"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.59)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.59)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuPSxl24ZKH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d168d1f3-a1e3-4ced-d29d-b2b5721c4d05"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m01ePr4c23-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3eb4b417-9adb-40e5-ebee-4408b22d3116"
      },
      "source": [
        "!pip install TextBlob"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from TextBlob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->TextBlob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmWW9NLuzocx",
        "colab_type": "text"
      },
      "source": [
        "## Install PySpark library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XafcDZPOpwTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "4bc48d03-b203-4b3b-808f-a7fa25a9cece"
      },
      "source": [
        "!pip install pyspark\n",
        "import os\n",
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "import pyspark.sql.functions as f"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 70kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=abdd401cdbc7e939d3e6504aae796f6168406181aaf8cf89859b09cfe12ec570\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak57zTsRqn_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar -xvf /content/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmyl-_DvrEUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um9eyxqVvT5U",
        "colab_type": "text"
      },
      "source": [
        "# Get Reddit-News data from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVO1NFkTuZKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ashfarhangi/Massive_Storage_and_Big_Data/master/data/Reddit-News.csv"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoSQJkeiTA3v",
        "colab_type": "text"
      },
      "source": [
        "# Top 10 most discussed topics from 2008-2016\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGPNJERMvcvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "00f6de2a-dc10-40b7-df48-2ab4aa3d5ea2"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# create list of stop words to use for filtering\n",
        "stop_words = gensim.parsing.preprocessing.STOPWORDS.union(set(['new', 'news', 'says']))\n",
        "\n",
        "# read in the reddt-news file and split it by line \n",
        "reddit_news = sc.textFile('Reddit-News.csv').map(lambda line: line.split(',', 1)[-1])\n",
        "\n",
        "# take each line in the RDD and split it by white space to get individual words\n",
        "words = reddit_news.flatMap(lambda line: line.lower().split(' '))\n",
        "\n",
        "# filter stopwords out and get count of all word occurences \n",
        "word_count = words.filter(lambda word: word not in stop_words and len(word) > 2).map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "# sort by count and take the top 10 entries\n",
        "most_common_words = word_count.map(lambda pair: (pair[1], pair[0])).sortByKey(False).take(10)\n",
        "\n",
        "# display the top 10 most discussed topics\n",
        "print('The top 10 most discussed topics are:')\n",
        "for count,pair in enumerate(most_common_words):\n",
        "  print('#{} topic: \"{}\" with {} occurences'.format(count+1, pair[-1], pair[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The top 10 most discussed topics are:\n",
            "#1 topic: \"police\" with 2567 occurences\n",
            "#2 topic: \"government\" with 2473 occurences\n",
            "#3 topic: \"people\" with 2324 occurences\n",
            "#4 topic: \"world\" with 1913 occurences\n",
            "#5 topic: \"u.s.\" with 1863 occurences\n",
            "#6 topic: \"china\" with 1759 occurences\n",
            "#7 topic: \"israel\" with 1722 occurences\n",
            "#8 topic: \"killed\" with 1720 occurences\n",
            "#9 topic: \"president\" with 1705 occurences\n",
            "#10 topic: \"war\" with 1695 occurences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC0RVlZEk4rI",
        "colab_type": "text"
      },
      "source": [
        "# Senitment Analysis using TextBlob library\n",
        "\n",
        "The following Sentiment Analysis of the Reddit-News data uses the TextBlob library and Pandas dataframe as well as the Pyspark dataframe with TextBlob for comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QceBhKtR69vo",
        "colab_type": "text"
      },
      "source": [
        "## Using Pandas Dataframe with TextBlob library for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itp82Z9XlOPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "6f6fe979-8a21-431c-dbbd-24441b2036ab"
      },
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# read in the reddit-news as a pandas dataframe \n",
        "reddit_news_df = pd.read_csv('Reddit-News.csv', parse_dates=True, index_col='Date')\n",
        "\n",
        "# apply the TextBlob sentiment analysis to each row containing news headline\n",
        "reddit_news_df['Sentiment Score'] = reddit_news_df['News'].apply(lambda headline: TextBlob(headline).sentiment.polarity) \n",
        "display(reddit_news_df)\n",
        "\n",
        "# get good and bad news count for each sentiment score\n",
        "sentiment_scores = reddit_news_df['Sentiment Score']\n",
        "good_news_count = reddit_news_df[reddit_news_df['Sentiment Score'] > 0].count()\n",
        "bad_news_count = reddit_news_df[reddit_news_df['Sentiment Score'] < 0].count()\n",
        "print('Good news count: \\n{}\\n'.format(good_news_count))\n",
        "print('Bad news count: \\n{}\\n'.format(bad_news_count))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>-0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-07-01</th>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-08</th>\n",
              "      <td>b'Man goes berzerk in Akihabara and stabs ever...</td>\n",
              "      <td>-0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-08</th>\n",
              "      <td>b'Threat of world AIDS pandemic among heterose...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-08</th>\n",
              "      <td>b'Angst in Ankara: Turkey Steers into a Danger...</td>\n",
              "      <td>-0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-08</th>\n",
              "      <td>b\"UK: Identity cards 'could be used to spy on ...</td>\n",
              "      <td>0.059091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-08</th>\n",
              "      <td>b'Marriage, they said, was reduced to the stat...</td>\n",
              "      <td>-0.083333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73608 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         News  Sentiment Score\n",
              "Date                                                                          \n",
              "2016-07-01  A 117-year-old woman in Mexico City finally re...        -0.066667\n",
              "2016-07-01   IMF chief backs Athens as permanent Olympic host         0.000000\n",
              "2016-07-01  The president of France says if Brexit won, so...         0.000000\n",
              "2016-07-01  British Man Who Must Give Police 24 Hours' Not...         0.111111\n",
              "2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...         0.000000\n",
              "...                                                       ...              ...\n",
              "2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...        -0.200000\n",
              "2008-06-08  b'Threat of world AIDS pandemic among heterose...         0.000000\n",
              "2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...        -0.600000\n",
              "2008-06-08  b\"UK: Identity cards 'could be used to spy on ...         0.059091\n",
              "2008-06-08  b'Marriage, they said, was reduced to the stat...        -0.083333\n",
              "\n",
              "[73608 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Good news count: \n",
            "News               21464\n",
            "Sentiment Score    21464\n",
            "dtype: int64\n",
            "\n",
            "Bad news count: \n",
            "News               17696\n",
            "Sentiment Score    17696\n",
            "dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt3M-G7c7gH3",
        "colab_type": "text"
      },
      "source": [
        "## Using PySpark dataframes with TextBlob library for sentiment analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRp-wcOY7ep4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# function used to return the sentiment of the passed in news headline\n",
        "def find_sentiment(news_headline):\n",
        "  sentiment_score = TextBlob(news_headline).sentiment.polarity\n",
        "  return sentiment_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdWvpqUQKRhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "24c751ae-1dc5-4b29-89a4-c5538f51fdda"
      },
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# read in reddit-news and display schema info\n",
        "reddit_news_df = spark.read.csv('Reddit-News.csv', inferSchema=True, header=True)\n",
        "reddit_news_df.printSchema()\n",
        "news_data = reddit_news_df.select('News')\n",
        "\n",
        "# create a user-defined function that will apply find_sentiment to passed in headlines\n",
        "sentiment_udf = f.udf(find_sentiment, DoubleType())\n",
        "spark.udf.register('sentiment', sentiment_udf)\n",
        "\n",
        "# get sentiment scores and create new column with sentiment scores for each headline\n",
        "news_data_with_sentiment = reddit_news_df.withColumn('Sentiment Score', sentiment_udf('News').cast('double'))\n",
        "news_data_with_sentiment.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- News: string (nullable = true)\n",
            "\n",
            "+----------+--------------------+--------------------+\n",
            "|      Date|                News|     Sentiment Score|\n",
            "+----------+--------------------+--------------------+\n",
            "|2016-07-01|A 117-year-old wo...|-0.06666666666666667|\n",
            "|2016-07-01|IMF chief backs A...|                 0.0|\n",
            "|2016-07-01|The president of ...|                 0.0|\n",
            "|2016-07-01|British Man Who M...| 0.11111111111111112|\n",
            "|2016-07-01|100+ Nobel laurea...|                 0.0|\n",
            "|2016-07-01|Brazil: Huge spik...|  0.4000000000000001|\n",
            "|2016-07-01|Austria's highest...|                -0.2|\n",
            "|2016-07-01|Facebook wins pri...|                0.25|\n",
            "|2016-07-01|Switzerland denie...|                 0.0|\n",
            "|2016-07-01|China kills milli...|                 0.5|\n",
            "|2016-07-01|France Cracks Dow...| -0.1277777777777778|\n",
            "|2016-07-01|Abbas PLO Faction...|                 0.0|\n",
            "|2016-07-01|Taiwanese warship...|                 0.0|\n",
            "|2016-07-01|Iran celebrates A...|                 0.0|\n",
            "|2016-07-01|U.N. panel moves ...|                 0.0|\n",
            "|2016-07-01|The United States...|                -0.5|\n",
            "|2016-07-01|S&amp;P revises E...|                 0.0|\n",
            "|2016-07-01|India gets $1 bil...|                 0.0|\n",
            "|2016-07-01|U.S. sailors deta...|                 0.2|\n",
            "|2016-07-01|Mass fish kill in...|                 0.0|\n",
            "+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}